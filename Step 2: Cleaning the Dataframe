## Cleaning the Dataframe
## Author: Neeraj Tom Savio (211863)

#Required Packages
#Installing these is a prerequisite for running the following code.
import pandas as pd
import string
import collections
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('punkt')
lemmatizer = nltk.stem.WordNetLemmatizer()
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
nltk.download('wordnet') 
nltk.download('vader_lexicon')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords 

##Importing the Dataframe
poems_df = pd.read_csv(r'/Users/neerajtomsavio/Downloads/poems_df.csv') #Insert the filepath after r'
#This converts the comma-separated values file into a dataframe using Pandas
#Name_of_the_dataframe (Whatever you would like to name it) = pd.read_csv(r'filepath)

##Checking the Dataframe
poems_df.head()
#Name_of_the_dataframe.head()
poems_df.info()
#Name_of_the_dataframe.info()
#This allows us to view the dataframe. 
#The .head function allows us to view the first 5 rows. This enables us to locate which columns to analyse. 
#The .info function shows how many NAs are in our dataframe and the type of values in our dataframe.

##Dropping NAs and converting column values to a string
poems_df['Content'] = poems_df['Content'].astype('string')
#Name['Column Name'] = NAME['Column Name'].astype('string') 
#This would convert the values of a particular column to string.
poems_df = poems_df.dropna()
#Name = Name.dropna() 
#This would remove any NA values from the dataframe.
#These steps are essential to enable further analysis. 
poems_df.info()
#Checking if the desired results have been achieved.

##Lowercasing 
poems_df['clean_poems'] = poems_df['Content'].str.lower()
#Name['Column for cleaned content'] = Name[Column with uncleaned content'].str.lower()
#This is to create a separate column for the clean contents. Helps to have a failsafe column (original).
poems_df['clean_poems'] = poems_df['clean_poems'].astype('string')
# Name['Column with cleaned content'] = Name['Column with cleaned content'].astype('string')
#Converting the cleaned column values to string
poems_df.head()
poems_df.info()
# Double-checking if the desired results have been achieved. 

##Removing Punctuation
poems_df['clean_poems'] = poems_df['clean_poems'].str.replace('[^\w\s]','')
#Name['Column with cleaned content'] = Name['Column with cleaned content'].str.replace('[^\w\s]','')
#Removing punctuation enables us to proceed with further cleaning.

##Removing Stopwords
stop = set(stopwords.words('english'))
#Define stopwords present in the NLTK library 
#Stopwords_Name = set(stopwords.words('english'))
poems_df['clean_poems'] = poems_df['clean_poems'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
#Name['Column with cleaned content'] = Name['Column with cleaned content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_name)]))
poems_df.head()
#Double-checking if desired results have been obtained.

##Lemmatisation and Tokenisation
poems_df = poems_df.dropna()
#Drop any NAs

def lemmatize_text(clean_poems):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(clean_poems)]
poems_df['clean_poems'] = poems_df.clean_poems.apply(lemmatize_text)
#define the function to lemmatize tokenised text
#Name['Column with cleaned content'] = Name.Column with cleaned content.apply(lemmatize_text) #You can use your own defined function to lemmatise

##Converting Dataframe to XLS
poems2_df = pd.DataFrame(poems_df)
#Saving the dataframe
#NewName = pd.DataFrame(Name)
poems2_df.to_excel('Clean_Poems.xlsx', index=False, header=True)
#NewName.to_excel('Column with cleaned content', index=False, header=True)

